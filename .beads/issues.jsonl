{"id":"awesome-talon-81y","title":"Talon Command Discovery Engine","description":"## Talon Command Discovery Engine\n\nCrawl 302+ tracked Talon repos (auto-expanding), parse .talon files, index voice commands, and surface alternative spoken phrases across repos. Spiritual successor to archived Stolen Sugar project.\n\n### Architecture: 4 Independent Auto-Commit Workflows\n\nEach workflow is a standalone bot that wakes up on a schedule, reads committed files, does its job, validates output, and commits the result back. The repo is the message bus. No workflow depends on another running — they just consume whatever files are currently committed.\n\n```\ndiscover (weekly) --\u003e repos_full.json\nenrich  (weekly) --\u003e repos_full.json (metadata filled)\ncrawl   (weekly) --\u003e website/data/talon-files-raw.json\nparse-and-index (weekly) --\u003e website/data/commands-stats.json + website/public/data/commands-*.json\n```\n\nEach commit is self-contained. If one workflow fails, the others keep working with the last good data.\n\n### Design Principles\n- Each workflow is independent — no orchestrator, no shared state beyond committed files\n- Auto-commit pattern (same as existing enrich-dates.yml)\n- Incremental by default — only re-process what changed\n- Scripts in top-level scripts/ directory, stdlib-only Python\n- Website page only reads pre-built JSON, all work done async\n\n### File Ownership\n- scripts/discover_repos.py --\u003e repos_full.json (adds new repos)\n- .playground/enrich.py --\u003e repos_full.json (fills metadata)\n- scripts/crawl_talon_files.py --\u003e website/data/talon-files-raw.json\n- scripts/parse_and_index.py --\u003e website/data/commands-stats.json + website/public/data/commands-*.json\n\n### References\n- https://github.com/stolen-sugar/stolen-sugar (archived, lessons learned)\n- https://github.com/wenkokke/tree-sitter-talon (formal grammar)\n- https://talon.wiki/Customization/basic_customization/","status":"open","priority":2,"issue_type":"epic","owner":"trillium@trilliumsmith.com","created_at":"2026-02-17T13:22:41.068969139-08:00","created_by":"Trillium Smith","updated_at":"2026-02-19T11:16:14.666214972-08:00"}
{"id":"awesome-talon-81y.1","title":"Step 1: Python crawler — fetch .talon file trees from GitHub API","description":"Write a Python script (scripts/crawl_talon_files.py) that:\n1. Reads repos_full.json for the list of repos to crawl\n2. For each repo, calls GitHub Trees API (GET /repos/{owner}/{repo}/git/trees/{branch}?recursive=1) to list all files\n3. Filters for .talon files\n4. Fetches file content via Blobs API (GET /repos/{owner}/{repo}/git/blobs/{sha})\n5. Caches fetched content by blob SHA in a local cache dir (content-addressable)\n6. Supports incremental mode: skip repos not pushed since last run (compare pushed_at to last crawl timestamp)\n7. Outputs raw .talon content to talon_files_raw.json -- array of {repo, path, sha, content, context_header}\n\nScripts live in top-level scripts/ directory (new directory, not .playground or .github/scripts).\n\n## Verify\n- [ ] Script runs without errors against live GitHub API\n- [ ] Produces talon_files_raw.json with entries from at least 50 repos\n- [ ] Incremental mode skips unchanged repos on second run\n- [ ] Respects GitHub API rate limits (5,000/hr)\n- [ ] Cache directory stores files by blob SHA, reused across runs","status":"tombstone","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-17T13:22:53.320455944-08:00","created_by":"Trillium Smith","updated_at":"2026-02-19T11:15:59.005189168-08:00","deleted_at":"2026-02-19T11:15:59.005189168-08:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"awesome-talon-81y.10","title":"Step 10: Seed data — run initial crawl and commit commands_index.json","description":"Run the full pipeline and seed the repo with initial data:\n1. Run the full pipeline locally to generate the first commands_index.json\n2. Validate the output: check file size, command count, parse success rate\n3. Commit commands_index.json to website/data/ so the site builds without CI\n4. Document any issues found during the initial crawl (update epic if needed)\n5. Verify the website builds and /commands page renders with real data\n\n## Verify\n- [ ] commands_index.json exists in website/data/ and is committed\n- [ ] File size is reasonable (under 10MB)\n- [ ] next build succeeds with the seeded data\n- [ ] /commands page renders with real commands from the ecosystem\n- [ ] At least 50 repos contributed commands to the index\n- [ ] Stats numbers match expectations from the research (8-10K files, 100K+ commands)","status":"tombstone","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-17T13:24:09.802987368-08:00","created_by":"Trillium Smith","updated_at":"2026-02-19T11:15:59.3725525-08:00","deleted_at":"2026-02-19T11:15:59.3725525-08:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"awesome-talon-81y.11","title":"Workflow 1: discover_repos.py + discover-repos.yml","description":"A standalone script + GitHub Action that discovers new Talon repos and merges them into repos_full.json.\n\n### Script: scripts/discover_repos.py\nRuns 4 GitHub searches (~70 seconds total, different rate limit buckets can overlap):\n1. Code search: extension:talon (10 pages, ~63s, 10 req/min limit)\n2. Topic search: topic:talonvoice + topic:talon (2 calls, ~2s)\n3. Language search: language:talon (1 call, ~1s)\n4. Fork listing: talonhub/community forks (9 pages, ~9s)\n\nMerges results into repos_full.json:\n- New repos added with name + url + discovered_at + source fields, empty metadata\n- Existing repos untouched (enrich workflow fills metadata)\n- Deduplicates by repo slug (owner/repo)\n- Tracks discovery source per repo: \"code_search\", \"topic\", \"language\", \"community_fork\" (can have multiple)\n- Logs how many new repos found\n\nUses gh api CLI (same as enrich.py). Stdlib only.\n\n### Ecosystem metrics (tracked in discover_stats.json)\nThe discover step is the source of truth for overall ecosystem size:\n- total_repos_discovered: total unique repos across all 4 searches\n- total_community_forks: number of talonhub/community forks (proxy for total Talon users)\n- repos_by_source: {code_search: N, topic: N, language: N, community_fork: N}\n- new_repos_this_run: how many were added this run\n- last_discovered: timestamp\n\nThis file is committed alongside repos_full.json and consumed by the website for the stats banner.\n\n### Workflow: .github/workflows/discover-repos.yml\n- Schedule: daily (midnight UTC — cheap, most runs find 0-2 new repos)\n- Also: workflow_dispatch for manual trigger\n- Steps: checkout -\u003e setup Python -\u003e run discover_repos.py -\u003e validate -\u003e commit if changed\n- Commit message: \"Discover N new Talon repos (M total tracked) [automated]\" or no commit if nothing new\n- Uses secrets.GITHUB_TOKEN for gh api\n\n### File contract\n- Reads: .playground/repos_full.json (current repo list)\n- Writes: .playground/repos_full.json (with new repos appended)\n- Writes: .playground/discover_stats.json (ecosystem metrics)\n\n## Verify\n- [ ] Script runs in under 2 minutes\n- [ ] Finds all currently known 302 repos (no regressions)\n- [ ] Discovers community forks (825+)\n- [ ] New repos have discovered_at timestamp and source field\n- [ ] Existing repos are not modified\n- [ ] discover_stats.json has accurate counts including total_community_forks\n- [ ] repos_full.json is valid JSON after merge\n- [ ] Workflow commits only when new repos found\n- [ ] Workflow runs successfully via manual trigger","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-19T11:16:29.998399657-08:00","created_by":"Trillium Smith","updated_at":"2026-02-19T12:28:27.513290779-08:00","dependencies":[{"issue_id":"awesome-talon-81y.11","depends_on_id":"awesome-talon-81y","type":"parent-child","created_at":"2026-02-19T11:16:30.001380371-08:00","created_by":"Trillium Smith"}]}
{"id":"awesome-talon-81y.12","title":"Workflow 2: Decouple enrich.py into standalone enrich-repos.yml","description":"The existing .playground/enrich.py already enriches repo metadata via GitHub GraphQL API. Currently it runs embedded inside deploy-website.yml. Decouple it into its own auto-commit workflow.\n\n### Workflow: .github/workflows/enrich-repos.yml\n- Schedule: daily (2am UTC — after discover runs at midnight)\n- Also: workflow_dispatch for manual trigger\n- Steps: checkout -\u003e setup Python -\u003e run enrich.py -\u003e validate -\u003e commit if changed\n- Commit message: \"Enrich repo metadata for N repos [automated]\"\n- Most daily runs do nothing if no repos have new pushes (enrich.py already checks pushed_at \u003e last_fetched)\n\n### Changes to deploy-website.yml\n- Remove the enrich.py step from deploy\n- Deploy just reads whatever repos_full.json is committed (already enriched by this workflow)\n- This makes deploy faster and decoupled\n\n### No script changes needed\nenrich.py already:\n- Reads repos_full.json\n- Skips repos not pushed since last fetch (incremental via pushed_at \u003e last_fetched)\n- Writes enriched repos_full.json\n- Uses gh api for GraphQL batch queries (50 repos/request)\n\nThe only new work is the workflow YAML and removing the enrich step from deploy.\n\n### File contract\n- Reads: .playground/repos_full.json (may have new unenriched repos from discover)\n- Writes: .playground/repos_full.json (with metadata filled: stars, topics, pushed_at, etc.)\n\n## Verify\n- [ ] Workflow runs standalone and commits enriched data\n- [ ] New repos discovered by workflow 1 get their metadata filled\n- [ ] Existing repos skip enrichment if unchanged (incremental)\n- [ ] Daily run with 0 changed repos completes quickly with no commit\n- [ ] deploy-website.yml still works without the enrich step (reads committed data)\n- [ ] No regression in repo count or metadata quality","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-19T11:16:42.374182652-08:00","created_by":"Trillium Smith","updated_at":"2026-02-19T12:29:40.172290647-08:00","dependencies":[{"issue_id":"awesome-talon-81y.12","depends_on_id":"awesome-talon-81y","type":"parent-child","created_at":"2026-02-19T11:16:42.377063873-08:00","created_by":"Trillium Smith"}]}
{"id":"awesome-talon-81y.13","title":"Workflow 3: crawl_talon_files.py + crawl-talon.yml","description":"A standalone script + GitHub Action that fetches .talon file content from all tracked repos.\n\n### Script: scripts/crawl_talon_files.py\n\nCRITICAL: This is a MERGE operation, not an overwrite. The script must:\n1. Read existing talon-files-raw.json (the full corpus from previous runs)\n2. Determine which repos need re-crawling (pushed_at \u003e pushed_at_when_crawled in crawl_state.json)\n3. Crawl only those repos\n4. Merge new/updated entries INTO the existing data\n5. Remove entries for repos that are archived or no longer in repos_full.json\n6. Write the merged result\n\nThis ensures a partial crawl (10 repos pushed this week) adds to the existing corpus of 300+ repos instead of replacing it.\n\nFor each repo that needs crawling:\n1. Fetch git tree via gh api /repos/{owner}/{repo}/git/trees/{branch}?recursive=1\n2. Filter for .talon files\n3. Fetch blob content via gh api (base64 decode)\n4. Cache blobs by SHA in scripts/.cache/blobs/{sha}.talon (content-addressable)\n\n### Merge logic\n- Key: (repo, path) tuple uniquely identifies a .talon file entry\n- On crawl: replace all entries for a re-crawled repo with fresh data\n- On removal: drop all entries for repos not in repos_full.json or marked archived\n- On no-change: entries for repos NOT re-crawled pass through untouched\n- Track per-repo state in crawl_state.json\n\n### Crawl cooldown (daily runs, per-repo freshness)\ncrawl_state.json tracks per-repo state:\n{\n  \"talonhub/community\": {\n    \"last_crawled\": \"2026-02-18T04:00:00Z\",\n    \"pushed_at_when_crawled\": \"2026-02-17T15:30:00Z\",\n    \"file_count\": 218\n  }\n}\n\nDecision logic: crawl repo if pushed_at (from repos_full.json) \u003e pushed_at_when_crawled (from crawl_state.json). This means:\n- The workflow runs daily but most runs do nothing (no repos pushed)\n- Active repos get re-crawled within 24 hours of a push\n- Stale repos are crawled once and never again until someone pushes\n- A daily run with 0 changed repos takes seconds (just reads JSON, compares timestamps, exits)\n- --full flag ignores cooldown and re-crawls everything\n\n### Repos with .talon files tracking\nWhen crawling a repo tree, log whether it contained any .talon files. Repos with zero .talon files are noted in crawl_state.json (has_talon_files: false) so we can report accurate ecosystem metrics. This helps answer \"how many people actually use Talon\" — not every fork or tagged repo actually has .talon files.\n\n### Output\nwebsite/data/talon-files-raw.json — array of {repo, path, blob_sha, content}\nThis file is the FULL corpus, always complete, never partial.\n\n### Key optimizations\n- Incremental: only crawl repos where pushed_at advanced\n- Blob SHA cache: forks sharing community files resolve to cached blobs, zero extra API calls\n- Rate limiting: time.sleep(0.05) between calls, check rate_limit endpoint periodically\n- Skip archived repos (remove their entries from the corpus)\n\n### Workflow: .github/workflows/crawl-talon.yml\n- Schedule: daily (4am UTC)\n- Also: workflow_dispatch with full_crawl boolean input\n- Steps: checkout -\u003e setup Python -\u003e restore cache (actions/cache for scripts/.cache/) -\u003e run crawl -\u003e validate -\u003e commit if changed\n- Commit message: \"Crawl .talon files: N repos updated, M total [automated]\"\n- actions/cache key: talon-blob-cache-{hash of repos_full.json}\n- No commit if nothing changed (most daily runs)\n\n### File contract\n- Reads: .playground/repos_full.json (enriched repo list with default_branch, pushed_at)\n- Reads: website/data/talon-files-raw.json (existing corpus to merge into)\n- Writes: website/data/talon-files-raw.json (merged full corpus)\n- Cache: scripts/.cache/ (blob cache + crawl_state.json, persisted via actions/cache)\n\n### Validation before commit\n- Output file is valid JSON\n- Total entry count \u003e= 80% of previous run (guard against accidental mass deletion)\n- Log: \"N repos crawled, M entries updated, K entries removed, T total entries\"\n\n## Verify\n- [ ] Script reads existing talon-files-raw.json and merges, does not overwrite\n- [ ] Incremental crawl of 10 repos preserves entries from the other 290+\n- [ ] Daily run with 0 changed repos completes in seconds with no commit\n- [ ] Active repo pushed today gets re-crawled on next daily run\n- [ ] Stale repo not pushed in months is never re-crawled\n- [ ] --full flag re-crawls everything\n- [ ] Archived repos have their entries removed\n- [ ] Repos removed from repos_full.json have their entries removed\n- [ ] has_talon_files tracked per repo in crawl_state.json\n- [ ] Blob SHA cache prevents re-downloading identical content across forks\n- [ ] Validation gate catches if output shrinks dramatically (\u003e 20% drop)\n- [ ] Workflow commits updated raw file only when changed\n- [ ] actions/cache persists blob cache between CI runs\n- [ ] Handles deleted/private repos gracefully (remove entries, dont crash)","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-19T11:16:59.980205946-08:00","created_by":"Trillium Smith","updated_at":"2026-02-19T12:29:24.02864443-08:00","dependencies":[{"issue_id":"awesome-talon-81y.13","depends_on_id":"awesome-talon-81y","type":"parent-child","created_at":"2026-02-19T11:16:59.981983107-08:00","created_by":"Trillium Smith"}]}
{"id":"awesome-talon-81y.14","title":"Workflow 4: parse_and_index.py + parse-and-index.yml","description":"A standalone script + GitHub Action that parses crawled .talon files and builds the command index JSON for the website. This combines parse + index since both are local CPU work with no API calls.\n\n### Script: scripts/parse_and_index.py\n\n#### Parse phase\nRead talon-files-raw.json, for each .talon file:\n1. Split on - separator (context header above, body below)\n2. Parse context header: extract os, app, tag, mode\n3. Parse body: extract spoken_form: action_body commands\n4. Handle multi-line actions (indented continuation)\n5. Skip settings() blocks, tag() declarations, comments\n6. Gracefully skip unparseable files, log warnings\n\n#### Index phase\n1. Group commands by normalized action body to find alternative spoken forms\n2. Mark canonical form (from talonhub/community)\n3. Mark unique commands (in 1-2 repos, not community)\n4. Deduplicate verbatim copies (forks with identical content)\n5. Cap repo lists at 10 per spoken form in detail files (\"and N more\")\n\n### Output: 3 artifacts\n1. website/data/commands-stats.json — small stats for build-time SSR\n   {totalCommands, uniqueSpokenForms, uniqueActionBodies, reposWithCommands, reposWithTalonFiles, totalCommunityForks (from discover_stats.json), communityCommandCount, lastIndexed, topContexts, most_creative_repos}\n\n   reposWithTalonFiles: count of repos in talon-files-raw.json that had at least 1 parseable command. This is the key \"how many Talon users\" metric.\n\n2. website/public/data/commands-summary.json — client-fetched for search\n   [{id, spokenForms, canonicalForm, actionPreview, repoCount, isUnique, contexts, letter, isCommunity}]\n\n3. website/public/data/commands-detail-{a-z,_misc}.json — on-demand detail per letter\n   {[id]: {spokenForms: [{form, repos[], context, filePath}], actionBody, repoCount, isUnique, isCommunity}}\n\n### Ecosystem stats passthrough\nThis step also reads .playground/discover_stats.json and merges its metrics (total_community_forks, total_repos_discovered) into commands-stats.json. This way the website only needs to read one stats file.\n\n### Workflow: .github/workflows/parse-and-index.yml\n- Schedule: daily (6am UTC — after crawl, cheap when input unchanged)\n- Also: workflow_dispatch for manual trigger\n- Only does real work when talon-files-raw.json has changed since last run (checksum comparison)\n- Steps: checkout -\u003e setup Python -\u003e run parse_and_index.py -\u003e validate JSON -\u003e validate file sizes -\u003e commit if changed\n- Commit message: \"Update command index: N commands from M repos [automated]\"\n\n### Validation gates (before commit)\n- All output files are valid JSON\n- commands-summary.json \u003c 10MB\n- Each detail chunk \u003c 1MB\n- totalCommands \u003e 0 in stats\n- Parse success rate logged to stdout\n\n### File contract\n- Reads: website/data/talon-files-raw.json (from crawl workflow)\n- Reads: .playground/discover_stats.json (ecosystem metrics from discover workflow)\n- Writes: website/data/commands-stats.json + website/public/data/commands-*.json\n\n## Verify\n- [ ] Parses talonhub/community files correctly (~2500 commands)\n- [ ] Parse success rate \u003e= 90% across full corpus\n- [ ] Grouped commands show alternatives from multiple repos\n- [ ] Canonical (community) vs alternative distinction works\n- [ ] Unique commands flagged correctly (1-2 repos, not community)\n- [ ] reposWithTalonFiles count is accurate\n- [ ] totalCommunityForks pulled from discover_stats.json\n- [ ] Summary \u003c 3MB gzipped, detail chunks \u003c 500KB each\n- [ ] IDs consistent between summary and detail files\n- [ ] Workflow commits only when output changed\n- [ ] Handles empty/missing talon-files-raw.json gracefully","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-19T11:17:24.870750081-08:00","created_by":"Trillium Smith","updated_at":"2026-02-19T12:28:56.209808773-08:00","dependencies":[{"issue_id":"awesome-talon-81y.14","depends_on_id":"awesome-talon-81y","type":"parent-child","created_at":"2026-02-19T11:17:24.872398065-08:00","created_by":"Trillium Smith"}]}
{"id":"awesome-talon-81y.15","title":"Seed data: run all 4 scripts locally and commit initial output","description":"## What\nRun the full pipeline locally to seed the repo with initial data so the website can be built.\n\n1. Run scripts/discover_repos.py — expand repos_full.json with forks + new discoveries\n2. Run .playground/enrich.py — fill metadata for any new repos\n3. Run scripts/crawl_talon_files.py — fetch .talon files\n4. Run scripts/parse_and_index.py — build the command index\n\nCommit all output files:\n- .playground/repos_full.json (expanded)\n- website/data/talon-files-raw.json\n- website/data/commands-stats.json\n- website/public/data/commands-summary.json\n- website/public/data/commands-detail-*.json\n- scripts/.gitignore (for .cache/)\n\nAfter commit, the website should build with real data on /commands.\n\n## Verify\n- [ ] All 4 scripts run without errors\n- [ ] repos_full.json has more than 302 repos (forks discovered)\n- [ ] talon-files-raw.json has entries from 50+ repos\n- [ ] commands-stats.json shows reasonable numbers\n- [ ] commands-summary.json \u003c 10MB\n- [ ] next build succeeds with seeded data\n- [ ] /commands page renders with real commands","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-19T11:17:35.995570008-08:00","created_by":"Trillium Smith","updated_at":"2026-02-19T11:17:35.995570008-08:00","dependencies":[{"issue_id":"awesome-talon-81y.15","depends_on_id":"awesome-talon-81y","type":"parent-child","created_at":"2026-02-19T11:17:35.997360127-08:00","created_by":"Trillium Smith"}]}
{"id":"awesome-talon-81y.2","title":"Step 2: .talon file parser — extract voice commands","description":"Write a Python module (scripts/parse_talon.py) that:\n1. Parses .talon file format: context header (above - separator) + command rules (below)\n2. Extracts each command as {spoken_rule, action_body, context}\n3. Handles rule syntax: alternatives (a|b), optionals [word], captures \u003cuser.text\u003e, lists {user.list}\n4. Normalizes context headers (e.g. os: mac, tag: user.cursorless, app: vscode)\n5. Start with regex parsing (handles ~95% of files)\n6. Gracefully skip unparseable files, log warnings\n\n## Verify\n- [ ] Correctly parses talonhub/community .talon files (218 files, ~2500 commands)\n- [ ] Unit tests cover: basic commands, multi-line actions, context headers, rule syntax variants\n- [ ] Parse success rate \u003e= 90% across full corpus\n- [ ] Unparseable files logged but do not crash the script","status":"tombstone","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-17T13:23:04.625958228-08:00","created_by":"Trillium Smith","updated_at":"2026-02-19T11:15:59.080216887-08:00","deleted_at":"2026-02-19T11:15:59.080216887-08:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"awesome-talon-81y.3","title":"Step 3: Command indexer — group by action, find alternatives","description":"Write a Python module (scripts/index_commands.py) that:\n1. Reads parsed commands from step 2\n2. Groups commands by normalized action body to find alternative spoken forms across repos\n3. Identifies the canonical form from talonhub/community (the entry point for most users, fine to treat as canonical)\n4. Deduplicates commands copied verbatim from community (same blob SHA or identical content)\n5. Computes stats: total unique commands, repos with most custom commands, most-overridden commands\n6. Marks commands as \"unique\" when they appear in only 1-2 repos and are NOT from talonhub/community — these are novel/creative commands someone invented\n\n## TWO output artifacts (required by hybrid frontend architecture):\n\n### commands-summary.json (lightweight, for search + list display)\n- Array of {id, rule, context, repoSlug, isCanonical, isUnique, repoCount} for ALL commands\n- isUnique: true when this action exists in only 1-2 repos and is not a community command\n- repoCount: how many repos define this action (enables sorting by \"most common\" or \"most unique\")\n- Target: ~5-10 MB raw, ~1-2 MB gzipped\n\n### commands-detail-{chunk}.json (chunked, for expand/drill-down)\n- Chunked by first letter of spoken rule (a.json, b.json, ... misc.json)\n- Each chunk contains: {id, actionBody, alternatives: [{rule, repo, context, path}], filePath}\n- Each chunk target: ~200-500 KB\n\n### commands-stats.json (small metadata)\n- {total_commands, total_repos_with_talon, total_files_parsed, total_unique_actions, total_unique_commands, most_overridden_commands: [{action, count}], most_creative_repos: [{repo, unique_count}], generated_at}\n\n## Verify\n- [ ] All three artifact types are produced and valid JSON\n- [ ] Summary file is under 3MB gzipped\n- [ ] Detail chunks exist for each letter with commands\n- [ ] Canonical vs alternative distinction works (community commands marked isCanonical)\n- [ ] Unique commands correctly identified (appear in 1-2 repos, not community)\n- [ ] repoCount is accurate for each command group\n- [ ] Stats include most_overridden and most_creative_repos leaderboards\n- [ ] IDs are consistent between summary and detail files (can join them)","status":"tombstone","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-17T13:23:18.113335297-08:00","created_by":"Trillium Smith","updated_at":"2026-02-19T11:15:59.148660024-08:00","deleted_at":"2026-02-19T11:15:59.148660024-08:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"awesome-talon-81y.4","title":"Step 4: Pipeline orchestrator — single entry point for CI","description":"Write a main script (scripts/build_commands_index.py) that:\n1. Orchestrates steps 1-3 in sequence: crawl -\u003e parse -\u003e index\n2. Accepts CLI flags: --full (ignore cache, recrawl everything), --repos-file (path to repos_full.json)\n3. Writes outputs to website/:\n   - website/data/commands-stats.json (read at build time by server component)\n   - website/public/data/commands-summary.json (fetched at runtime by client)\n   - website/public/data/commands-detail-{a-z,misc}.json (fetched on demand)\n4. Prints summary to stdout (total repos crawled, files parsed, commands indexed, errors)\n5. Exits with nonzero code on critical failure (e.g. no GitHub token, API down)\n6. Handles GITHUB_TOKEN from environment (same pattern as existing enrich.py)\n7. Uses only stdlib (no pip install needed — match existing script conventions)\n\nScripts live in top-level scripts/ directory.\n\n## Verify\n- [ ] Running the script end-to-end produces all output files\n- [ ] Summary output is printed to stdout\n- [ ] --full flag forces complete recrawl\n- [ ] Script fails gracefully with clear error if GITHUB_TOKEN is missing\n- [ ] Exit code is 0 on success, nonzero on failure\n- [ ] Output files are valid JSON and correctly split (summary vs detail vs stats)","status":"tombstone","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-17T13:23:29.197267677-08:00","created_by":"Trillium Smith","updated_at":"2026-02-19T11:15:59.228830208-08:00","deleted_at":"2026-02-19T11:15:59.228830208-08:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"awesome-talon-81y.5","title":"Step 5: GitHub Action — weekly CI for command index","description":"The command index should be generated by a GitHub Action that runs periodically and commits the result back to the repo (same pattern as enrich-dates.yml, NOT embedded in the deploy workflow).\n\nCreate .github/workflows/crawl-commands.yml:\n1. Runs on a weekly schedule (Monday 6am UTC) + manual workflow_dispatch trigger\n2. Work is performed in batches — the crawler processes repos in batches to stay within API rate limits\n3. Steps: checkout -\u003e setup Python -\u003e run scripts/build_commands_index.py -\u003e verify output -\u003e commit + push\n4. On completion, commits commands_index.json back to the repo (like enrich_dates.py auto-commits resource_dates.json)\n5. GITHUB_TOKEN provided via secrets.GITHUB_TOKEN\n6. Cache the blob SHA cache directory between CI runs (actions/cache) for incremental crawling\n7. The deploy-website.yml workflow picks up the committed commands_index.json on next deploy — no coupling between workflows\n\n## Verification is built into the workflow\n- Verify commands_index.json is valid JSON before committing\n- Verify file size is under 10MB\n- Verify command count is nonzero\n- Log a summary: repos crawled, files parsed, commands indexed, errors\n- Only commit if the file actually changed (diff check like enrich-dates.yml)\n\n## Verify\n- [ ] Workflow runs successfully via manual trigger\n- [ ] commands_index.json is committed back to the repo automatically\n- [ ] Cache persists between runs (incremental crawling works)\n- [ ] Workflow completes within GitHub Actions time limits\n- [ ] deploy-website.yml still works independently (picks up committed data)\n- [ ] Bad data is never committed (validation gate prevents corrupt output)","status":"tombstone","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-17T13:23:39.306544122-08:00","created_by":"Trillium Smith","updated_at":"2026-02-19T11:15:59.29981742-08:00","deleted_at":"2026-02-19T11:15:59.29981742-08:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"awesome-talon-81y.6","title":"Step 6: Data loader — load-commands.ts library module","description":"Create website/src/lib/load-commands.ts following existing patterns (load-repos.ts, load-resource-dates.ts):\n\n## Server-side (build time)\n1. Reads commands-stats.json at build time for the page shell (stats banner, metadata)\n2. Exports loadCommandStats(): CommandStats\n3. Graceful fallback: returns empty/zero stats if file missing\n\n## Client-side (runtime)\nThe summary and detail files live in public/ and are fetched at runtime, NOT loaded at build time (too large for SSR props).\nThe data loader should export:\n- fetchCommandsSummary(): Promise\u003cCommandSummary[]\u003e — fetches commands-summary.json from public/\n- fetchCommandDetail(letter: string): Promise\u003cCommandDetail[]\u003e — fetches commands-detail-{letter}.json\n\n## Types to export\n- CommandSummary: {id, rule, context, repoSlug, isCanonical}\n- CommandDetail: {id, actionBody, alternatives: CommandAlternative[], filePath}\n- CommandAlternative: {rule, repo, context, path}\n- CommandStats: {total_commands, total_repos_with_talon, total_files_parsed, total_unique_actions, generated_at}\n\n## Verify\n- [ ] Types are well-defined and exported\n- [ ] loadCommandStats() returns typed data at build time\n- [ ] Returns empty/default stats when file is missing (no build crash)\n- [ ] fetchCommandsSummary() and fetchCommandDetail() work client-side\n- [ ] All types align with the Python indexer output schema","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-17T13:24:09.424751604-08:00","created_by":"Trillium Smith","updated_at":"2026-02-17T13:35:43.482143475-08:00","dependencies":[{"issue_id":"awesome-talon-81y.6","depends_on_id":"awesome-talon-81y","type":"parent-child","created_at":"2026-02-17T13:24:09.426947621-08:00","created_by":"Trillium Smith"}]}
{"id":"awesome-talon-81y.7","title":"Step 7: /commands page — server component shell","description":"Create website/src/app/commands/page.tsx:\n1. Server component that loads command stats via loadCommandStats() at build time\n2. Does NOT embed the full command data as props (too large for SSR)\n3. Passes only stats to the CommandsExplorer client component, which fetches summary/detail at runtime\n4. Includes page metadata: title \"Voice Commands - Awesome Talon\", description for SEO\n5. Stats banner showing ecosystem health:\n   - Voice commands indexed (totalCommands)\n   - Repos with .talon files (reposWithTalonFiles) — the \"how many Talon users\" number\n   - Community forks (totalCommunityForks) — proxy for total Talon interest\n   - Unique spoken forms (uniqueSpokenForms)\n   - Last updated timestamp\n6. Graceful empty state if commands-stats.json is missing or has zero counts\n\n## Verify\n- [ ] Page renders at /commands without errors\n- [ ] Stats banner shows accurate numbers including reposWithTalonFiles and totalCommunityForks\n- [ ] Empty state renders cleanly when no data is available\n- [ ] Page metadata is correct\n- [ ] next build succeeds with the new page (even without summary/detail files in public/)\n- [ ] The page does NOT bloat the static HTML (data fetched client-side)","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-17T13:24:09.51063441-08:00","created_by":"Trillium Smith","updated_at":"2026-02-19T12:29:54.011528186-08:00","dependencies":[{"issue_id":"awesome-talon-81y.7","depends_on_id":"awesome-talon-81y","type":"parent-child","created_at":"2026-02-17T13:24:09.512648985-08:00","created_by":"Trillium Smith"}]}
{"id":"awesome-talon-81y.8","title":"Step 8: CommandsExplorer — client component with search and filtering","description":"Create website/src/components/CommandsExplorer.tsx (client component).\n\n## Architecture: Hybrid Summary + Detail Loading\nThe page does NOT load the full dataset at once. Instead:\n1. On page load, fetch commands-summary.json (~1-2 MB gzipped) containing {id, rule, context, repoSlug, isCanonical, isUnique, repoCount} for all commands\n2. Build a MiniSearch index in-memory from summary data (~1-2s, show loading skeleton)\n3. Render results using TanStack Virtual (@tanstack/react-virtual) — only ~30 DOM nodes regardless of result count\n4. When user expands a command, fetch the detail chunk for full action body + alternatives\n\n## Dependencies to add\n- minisearch (~7KB gzip) — client-side full-text search\n- @tanstack/react-virtual (~12KB gzip) — virtualized rendering\n\n## UI Design (follows EcosystemFilter patterns)\nPrimary filters (always visible):\n- Search input: searches command voice pattern, action, context, AND repo name. Placeholder: \"Search commands, contexts, repos...\"\n- Context dropdown: \u003cselect\u003e with common contexts (All, VS Code, Browser, Terminal, Mac, Linux, Windows)\n\nSecondary filters:\n- Sort dropdown: Most alternatives, Alphabetical, Most unique\n- Checkbox: Hide duplicates / Show unique only\n- Checkbox: Show unique commands only (isUnique — commands in 1-2 repos, not from community)\n\nUnique commands get a visual badge/indicator (e.g. a \"Unique\" or \"Novel\" chip) so users can spot creative commands while browsing.\n\nCommand cards show:\n- Canonical spoken rule (highlighted, from talonhub/community)\n- Action body (code-formatted, loaded on expand)\n- Repo badge chip — clicking it applies repo:owner/name to search box\n- \"in N repos\" count badge, expandable to show alternatives\n- \"Unique\" badge when isUnique is true\n\n## Search qualifier syntax (progressive enhancement)\n- repo:talonhub/community — filter to a single repo\n- app:vscode — filter to VS Code context\n- os:mac — filter to Mac OS\n- Free text searches command patterns and descriptions\n\n## What NOT to build for v1\n- No searchable combobox for repos (text search handles it)\n- No faceted sidebar\n- No tag chips for multi-repo selection\n- No \"link my repo\" personalized view (Version B — follow-up ticket)\n\n## Build-time data pipeline (scripts/ responsibility)\nThe Python pipeline must produce TWO artifacts:\n1. commands-summary.json — lightweight, all records, searchable fields only + isUnique + repoCount\n2. commands-detail-{chunk}.json — chunked by letter/category, full record data\n\n## Verify\n- [ ] Loading skeleton shown while summary fetches + index builds\n- [ ] Search filters commands in real-time without lag (sub-millisecond via MiniSearch)\n- [ ] Context dropdown filters correctly\n- [ ] Sort options reorder results as expected\n- [ ] \"Show unique only\" filter works and surfaces novel community commands\n- [ ] Unique badge displays on commands in 1-2 repos\n- [ ] Virtual scrolling works — scrolling 100K results stays smooth (60fps)\n- [ ] Command expand loads detail chunk and shows action body + alternatives\n- [ ] repo:slug search qualifier works\n- [ ] Dark mode renders correctly\n- [ ] Total initial payload is under 3MB gzipped","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-17T13:24:09.585016718-08:00","created_by":"Trillium Smith","updated_at":"2026-02-17T13:54:11.922656255-08:00","dependencies":[{"issue_id":"awesome-talon-81y.8","depends_on_id":"awesome-talon-81y","type":"parent-child","created_at":"2026-02-17T13:24:09.587084486-08:00","created_by":"Trillium Smith"}]}
{"id":"awesome-talon-81y.9","title":"Step 9: Navigation — add Commands link to site nav","description":"Update site navigation:\n1. Add Commands link to the site navigation/header alongside existing pages\n2. Add a Commands section or link to the home page (similar to how Ecosystem and Curated List are linked)\n3. Update home page stats if relevant (e.g. add a Voice commands indexed stat)\n\n## Verify\n- [ ] Commands link appears in site navigation\n- [ ] Link navigates to /commands correctly\n- [ ] Active state works when on /commands page\n- [ ] Home page references the commands feature\n- [ ] All existing navigation still works","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-17T13:24:09.697368105-08:00","created_by":"Trillium Smith","updated_at":"2026-02-17T13:24:09.697368105-08:00","dependencies":[{"issue_id":"awesome-talon-81y.9","depends_on_id":"awesome-talon-81y","type":"parent-child","created_at":"2026-02-17T13:24:09.700514039-08:00","created_by":"Trillium Smith"}]}
{"id":"awesome-talon-cf4","title":"Compile common voice dictation word replacements list","description":"Build a reference list of words commonly misrecognized or replaced in voice dictation with Talon.\n\nKey discovery: talonhub/community has a built-in system for this:\n- settings/words_to_replace.csv — CSV file (auto-created on first run) where users map misrecognized words to correct ones. Format: target word first, spoken/misheard word second.\n- core/vocabulary/vocabulary.py — PhraseReplacer class that powers the substitution system. Also handles capitalization defaults (months, etc.) and custom vocabulary.\n- Users can add replacements by voice: 'copy to replacements as \u003cphrase\u003e' (edit_vocabulary.talon)\n- The file is per-user (in settings/ dir, gitignored by most people)\n\nIdea: Aggregate words_to_replace.csv files from:\n1. Notable personal configs in the awesome list (andreas-talon, colton_talon, etc.)\n2. Community Slack discussions about common misrecognitions\n3. The homophones.csv file (core/homophones/homophones.csv)\n\nPossible outputs:\n- A crowdsourced 'common replacements' reference on the website\n- A starter words_to_replace.csv that new users can drop in\n- Searchable dataset showing which words are most commonly confused\n\nExample: 'crash mark' → 'question mark' (speech engine substitution)","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-17T10:51:49.987228644-08:00","created_by":"Trillium Smith","updated_at":"2026-02-17T10:53:36.622744283-08:00"}
